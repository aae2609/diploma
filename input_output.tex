\subsection{Входные и выходные данные}\label{subsec:input_output}

\textbf{Входные параметры.} Нейронная сеть получает на~вход ошибочное кодовое слово $\widetilde{c} \in \widetilde{C}$,
где $\widetilde{c} = c \oplus e_j, c \in C, j \in \{1;\dots; n\}$,
представленное в~виде набора признаков $f = (f_1, \dots, f_n)$, где~$f_i$ равен значению $i$-го бита~$\widetilde{c}$.

\textbf{Пример.}
\begin{equation}
    \nonumber\widetilde{c} = (0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1)
\end{equation}
\begin{eqnarray}
 % Remove numbering (before each equation)
    \nonumber f_1 &=& 0, \\
    \nonumber f_2 &=& 0, \\
    \nonumber f_3 &=& 1, \\
    \nonumber &\cdots& \\
    \nonumber f_{15} &=& 1.
\end{eqnarray}

\textbf{Выходные параметры.} Предполагается, что на выходе будет получено слово $\hat{c}$, с некоторой вероятностью равное слову $c$.
Слово $\hat{c}$ представляется вектором вероятностей~$p$. Каждый элемент $p_i~(\in [0, 1], i \in \{1;\dots; n\})$ вектора $p$, определяет вероятность того, что на~$i$-ой позиции слова $\hat{c}$ находится бит, равный единице.

\textbf{Пример.} \\
Пусть дан вектор $p$.
$$p = (0.0, 0.1, 0.2, 0.3, 0.4, 0.49, 0.5, 0.51, 0.6, 0.7, 0.8, 0.9, 1.0, 0.5, 1.0, 1.0)$$
Тогда он определяет следующий вектор $\hat{c}$.
$$\hat{c} = (0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1).$$

Далее, согласно алгоритму, описанному в разделе~\ref{subsec:neural_network}, вычисляются значение функции потерь $f_{loss}$
и обновляются веса скрытых слоев $W_i$, где $i\in\{1;\dots; L\}$.
