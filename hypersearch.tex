\subsection{Выбор оптимальных гиперпараметров}\label{subsec:opt_hyper}

Для выбора лучшего набора гиперпараметров из~пространства $\mathbb{S}$ используем функцию fmin\cite[раздел FMin]{bib:hyperopt} из~библиотеки hyperopt.
Она перебирает наборы $s \in \mathbb{S}$, для~каждого из~них строит нейронную сеть $\mathcal{N}$, обучает~ее
и сохраняет результат проверки $r$ на валидационной выборке $D_V$ и историю обучения $h$ с помощью функции history\cite[раздел callbacks]{bib:keras}.
В~процессе поиска обнаружено, что нейронные сети, содержащие функции активации  $f_a \in \{f_{relu}; f_{elu}\}$, количества нейронов $K_i \in \{512; 1024; 2048\}$, функции потерь $f_{loss} \in \{f_{mse}; f_{mae}; f_{binary\_crossentropy}\}$ и с~количеством эпох, меньшим~$1000$, показывают низкое качество.

Произведем поиск гиперпараметров в подпространстве $\hat{S} \subset \mathbb{S}$, исключающем вышеперечисленные параметры.
В~результате получим список наборов гиперпараметров $\widehat{s} = (\widehat{s}_1, \widehat{s}_2, \dots, \widehat{s}_{|\hat{S}|})$ с результатами их проверок $r_i$,
где $\widehat{s}_i \in \hat{S}$~-~набор гиперпараметров, $i \in \{1; \dots; |\hat{S}|$\}.

В результате анализа полученного списка выявлено, что наилучшее качество достигается при следующем наборе гиперпараметров:
\begin{equation}\label{eq:hyper_set}
    s^* =
    \begin{cases}
        S = 5, \\
        K_i = 208, \\
        Z = 256, \\
        f_{loss} = f_{exp}, \\
        f_{a,i} = f_{sigmoid}.
    \end{cases}
\end{equation}


\newpage 